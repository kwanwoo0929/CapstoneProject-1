import requests
import pandas as pd
import os
import time
from tqdm import tqdm 

# --- 설정 ---
BASE_URL = "https://collectionapi.metmuseum.org/public/collection/v1/"
OUTPUT_DIR = "met_artworks" 
CSV_FILE = os.path.join(OUTPUT_DIR, "processed_artworks.csv")

# 폴더 생성
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"'{OUTPUT_DIR}' 폴더를 생성했습니다.")

# --- 1. 모든 부서 ID를 가져와 작품 ID를 분할 수집 ---
all_object_ids = set() # 중복 방지를 위해 set 사용

try:
    print("1-1. 모든 부서(Department) ID를 가져오는 중...")
    departments_response = requests.get(f"{BASE_URL}departments")
    departments_response.raise_for_status()
    departments_data = departments_response.json()
    
    department_ids = [dept['departmentId'] for dept in departments_data['departments']]
    print(f"총 {len(department_ids)}개의 부서를 찾았습니다.")

    print("\n1-2. 부서별 작품 ID를 분할하여 수집 중...")
    for dept_id in tqdm(department_ids, desc="부서별 ID 수집"):
        try:
            # 특정 부서 ID를 사용하여 작품 ID 목록 요청
            dept_url = f"{BASE_URL}objects?departmentIds={dept_id}"
            objects_response = requests.get(dept_url)
            objects_response.raise_for_status()
            objects_data = objects_response.json()
            
            # 작품 ID 목록을 set에 추가 (자동으로 중복 제거)
            current_ids = objects_data.get('objectIDs', [])
            all_object_ids.update(current_ids)
            
            time.sleep(0.05) # API Rate Limit 회피
            
        except requests.exceptions.RequestException as e:
            # print(f"  - 부서 ID {dept_id} 처리 중 오류 발생: {e}") 
            continue

    object_ids_list = list(all_object_ids)
    print(f"\n✅ 분할 수집을 통해 총 {len(object_ids_list)}개의 고유 작품 ID를 성공적으로 수집했습니다.")

except requests.exceptions.RequestException as e:
    print(f"초기 API 호출 중 치명적인 오류 발생: {e}")
    object_ids_list = []

# --- 2. 상세 정보 수집 및 이미지 다운로드 ---
artworks_list = []  # 메타데이터를 저장할 리스트
MAX_COLLECTION = 50 # 테스트를 위해 수집할 최대 작품 수 (전체를 원하면 len(object_ids_list)로 변경)

print(f"\n2. 상세 정보 수집 및 이미지 다운로드 중 (최대 {MAX_COLLECTION}개)...")

for object_id in tqdm(object_ids_list, desc="작품 상세 정보 처리"): 
    if len(artworks_list) >= 50: 
        break
    try:
        object_url = f"{BASE_URL}objects/{object_id}"
        artwork_response = requests.get(object_url)
        artwork_response.raise_for_status()
        artwork_data = artwork_response.json()
        
        image_url = artwork_data.get('primaryImage')
        
        if image_url:
            artwork_info = {
                'objectID': object_id,
                'title': artwork_data.get('title'),
                'artistDisplayName': artwork_data.get('artistDisplayName'),
                'objectDate': artwork_data.get('objectDate'),
                'primaryImage': image_url
            }
            artworks_list.append(artwork_info)
            image_filename = os.path.join(OUTPUT_DIR, f"{object_id}.jpg")
            if not os.path.exists(image_filename):
                image_data = requests.get(image_url, stream=True)
                image_data.raise_for_status()
                with open(image_filename, 'wb') as f:
                    for chunk in image_data.iter_content(chunk_size=8192):
                        f.write(chunk)
                        
        time.sleep(0.05) # API Rate Limit 회피를 위해 0.05초 대기
            
    except requests.exceptions.RequestException as e:
        # print(f"작품 ID {object_id} 처리 중 오류 발생: {e}") 
        continue

# --- 3. 데이터 전처리 및 CSV 저장 ---
print("\n3. 데이터 전처리 및 CSV 파일 저장 중...")
if artworks_list:
    df = pd.DataFrame(artworks_list)
    df['artistDisplayName'].fillna('Unknown', inplace=True)
    df['objectID'] = df['objectID'].astype(int)
    df.set_index('objectID', inplace=True)
    df.to_csv(CSV_FILE, encoding='utf-8')
    
    print(f"✅ 최종 {len(df)}개 작품의 메타데이터를 '{CSV_FILE}'에 저장했습니다.")
    print(f"✅ 이미지 파일은 '{OUTPUT_DIR}' 폴더에 저장되었습니다.")
    print("\n이제 다음 단계인 '비전 임베딩 모델'을 이용한 벡터 생성으로 넘어갈 수 있습니다.")
else:
    print("❌ 수집된 작품 데이터가 없거나, 모든 ID에서 오류가 발생했습니다.")

import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import pandas as pd
import numpy as np
import os
from tqdm import tqdm

# --- 설정 (이전 단계에서 사용한 경로와 동일해야 함) ---
BASE_PATH = os.getcwd()
OUTPUT_DIR = os.path.join(BASE_PATH, "met_artworks")
EMBEDDINGS_DIR = os.path.join(OUTPUT_DIR, "embeddings") 
CSV_FILE = os.path.join(OUTPUT_DIR, "processed_artworks.csv")

# 폴더 생성
if not os.path.exists(EMBEDDINGS_DIR):
    os.makedirs(EMBEDDINGS_DIR)
    print(f"'{EMBEDDINGS_DIR}' 폴더를 생성했습니다.")

try:
    print("1. CLIP 모델과 프로세서 로드 중...")
    
    # 안정적이고 검증된 모델 사용 (768차원 벡터 생성)
    model_name = "openai/clip-vit-base-patch32" 
    
    # use_fast=True로 설정하여 경고 제거 및 전처리 속도 향상
    processor = CLIPProcessor.from_pretrained(model_name, use_fast=False) 
    model = CLIPModel.from_pretrained(model_name)
    
    # 모델을 GPU(사용 가능 시) 또는 CPU로 이동
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"  - 모델 로드 완료 ({model_name}). 사용 장치: {device}")
    
except Exception as e:
    print(f"❌ 모델 로드 중 치명적인 오류 발생: {e}. 라이브러리 상태를 확인하세요.")
    exit()

try:
    # 1. 전처리된 메타데이터 CSV 파일 로드
    # 인덱스 컬럼을 objectID로 지정하고 UTF-8 인코딩 사용
    df = pd.read_csv(CSV_FILE, index_col='objectID', encoding='utf-8')
    print(f"2. 메타데이터 로드 완료. 총 {len(df)}개 작품 처리 예정.")
    
    # 2. 벡터 파일 경로를 저장할 새 컬럼이 없으면 생성 (⭐컬럼 생성 코드를 try 블록 안으로 이동⭐)
    # 이 컬럼이 메모리에서 생성된 df 객체에 즉시 추가되도록 합니다.
    if 'embedding_filepath' not in df.columns:
        # 빈 문자열로 초기화된 새 컬럼 생성
        df['embedding_filepath'] = '' 
        print("  - 'embedding_filepath' 컬럼이 성공적으로 생성되었습니다.")
        
except Exception as e:
    print(f"❌ CSV 파일 로드 중 오류 발생: {e}. 파일 경로를 확인하세요.")
    # 이 단계에서 오류가 발생하면 이후 코드 실행은 의미 없으므로, exit()를 사용해도 좋습니다.
    # exit()

import glob # 코드 상단에 추가되어야 합니다. (import glob)

print("\n3. 이미지 벡터 생성 및 저장 중...")
for object_id, row in tqdm(df.iterrows(), total=len(df), desc="벡터 생성 진행률"):
    
    # objectID가 인덱스일 수 있으므로 문자열로 변환하여 사용
    object_id_str = str(object_id) 
    embedding_filepath = os.path.join(EMBEDDINGS_DIR, f"{object_id_str}.npy")
    print(embedding_filepath)
    print(11111)
    # 1. 이미 파일이 있다면 건너뛰고 경로 업데이트
    if os.path.exists(embedding_filepath):
        df.loc[object_id_str, 'embedding_filepath'] = embedding_filepath
        print(123)
        continue

    # ⭐ 핵심 수정: objectID로 시작하는 모든 확장자 파일을 검색합니다.
    image_files = glob.glob(os.path.join(OUTPUT_DIR, f"{object_id_str}.*"))
    
    if not image_files:
        # 이미지가 없으면 건너뜁니다.
        print(123)
        continue
        
    # 찾은 파일 중 첫 번째 파일을 사용합니다. (예: 12345.jpg 또는 12345.png)
    image_path = image_files[0] 
    # ------------------------------------------------------------------------

    try:
        image = Image.open(image_path).convert("RGB")       
        inputs = processor(images=image, return_tensors="pt", padding=True)
        with torch.no_grad():
            inputs = {k: v.to(device) for k, v in inputs.items()}
            image_features = model.get_image_features(**inputs)
        vector = image_features.cpu().numpy().flatten()
        print(f"저장 시도 경로: {embedding_filepath}")
        np.save(embedding_filepath, vector)
        if not os.path.exists(embedding_filepath):
            continue
        df.loc[object_id, 'embedding_filepath'] = embedding_filepath

        # 메모리 정리 (생략)

    except Exception as e:
        print(f"\n❌ 심각 오류: 작품 ID {object_id_str} 처리 중 오류 발생. 원인: {e}")
        continue

# --- 3. 최종 CSV 파일 업데이트 (수정) ---
print("\n4. 최종 메타데이터 CSV 파일 업데이트 중...")

# ⭐ 핵심 수정: embedding_filepath 컬럼 값이 비어있지 않은(벡터 생성이 성공한) 행만 필터링
df_final = df[df['embedding_filepath'].str.len() > 0].copy()

# 최종적으로 남아 있는 작품 수로 CSV 저장
try:
    df_final.to_csv(CSV_FILE, encoding='utf-8')
    
    print(f"✅ 최종 {len(df_final)}개 작품의 벡터 생성 및 CSV 업데이트 완료.")
    print(f"이제 '{CSV_FILE}' 파일에 임베딩 벡터 파일 경로가 추가되었습니다.")
    print("\n다음 단계인 '벡터 검색 환경 구축'으로 넘어갈 수 있습니다.")
    
except PermissionError:
    print(f"\n❌ PermissionError: CSV 파일이 다른 프로그램에 열려있을 수 있습니다. 파일을 닫고 다시 시도해 주세요.")

df['embedding_filepath']
import numpy as np
import os

# 예시 파일 경로 (실제 파일 경로로 변경하세요!)
EMBEDDINGS_DIR = "met_artworks/embeddings" 
EXAMPLE_OBJECT_ID = "34" # 오류가 났던 작품 ID를 예시로 사용

# 파일 경로 조합
npy_filepath = os.path.join(EMBEDDINGS_DIR, f"{EXAMPLE_OBJECT_ID}.npy")

try:
    # NumPy를 사용하여 파일 로드
    vector_data = np.load(npy_filepath)
    
    print(f"파일 로드 성공: {npy_filepath}")
    print("   - 벡터 값 (상위 10개):")
    # 벡터는 보통 768차원이므로, 앞부분만 출력해봅니다.
    print(vector_data[:10]) 

except FileNotFoundError:
    print(f"❌ 오류: 파일을 찾을 수 없습니다. 경로를 확인하세요: {npy_filepath}")
except Exception as e:
    print(f"❌ 오류: 파일 로드 중 문제가 발생했습니다: {e}")
