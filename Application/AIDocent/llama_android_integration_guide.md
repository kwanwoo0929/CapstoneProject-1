# Androidì—ì„œ **llama.cpp** í†µí•© ê°€ì´ë“œ (LMPlayground ê¸°ë°˜)

ìƒì„±ì¼: 2025-11-03 02:18

ì´ ë¬¸ì„œëŠ” ì—…ë¡œë“œí•˜ì‹  í”„ë¡œì íŠ¸ **LMPlayground**ë¥¼ ì‹¤ì œë¡œ ë¶„ì„í•˜ì—¬ ì •ë¦¬í•œ, **ì•ˆë“œë¡œì´ë“œ ì•±ì— llama.cppë¥¼ í†µí•©í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œ**ì…ë‹ˆë‹¤.
ê²½ë¡œì™€ API ì´ë¦„, Gradle/NDK/CMake ì„¤ì •ì€ ëª¨ë‘ ë¦¬í¬ì˜ ì‹¤ì œ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ§© 1. í”„ë¡œì íŠ¸ ê°œìš”

### íŠ¸ë¦¬(í•µì‹¬ ê²½ë¡œë§Œ)
```
LMPlayground/
â””â”€ app/
   â””â”€ src/main/cpp/
      â”œâ”€ llama.cpp/                # llama.cpp ì†ŒìŠ¤(ì„œë¸Œë””ë ‰í„°ë¦¬ë¡œ í¬í•¨)
      â”œâ”€ CMakeLists.txt            # native ë¹Œë“œ ì •ì˜
      â”œâ”€ LlamaCpp.h
      â”œâ”€ LlamaModel.cpp
      â””â”€ native-lib.cpp            # JNI ë¸Œë¦¬ì§€ êµ¬í˜„
â””â”€ app/build.gradle.kts            # ì•± ëª¨ë“ˆ Gradle
â””â”€ build.gradle.kts                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ Gradle
â””â”€ settings.gradle.kts
â””â”€ app/src/main/java/
   â””â”€ com/druk/llamacpp/           # Kotlin JNI wrapper íŒ¨í‚¤ì§€
      â”œâ”€ LlamaCpp.kt               # native ë©”ì„œë“œ ì„ ì–¸(loadModel ë“±)
      â”œâ”€ LlamaModel.kt             # ëª¨ë¸ í•¸ë“¤ / ì„¸ì…˜ ìƒì„± ë“±
      â”œâ”€ LlamaGenerationSession.kt # ìƒì„± ì„¸ì…˜ JNI ë˜í¼
      â”œâ”€ LlamaGenerationCallback.kt
      â””â”€ LlamaProgressCallback.kt
â””â”€ app/src/main/java/com/druk/lmplayground/
   â””â”€ ...                          # ìƒ˜í”Œ UI, ViewModel, ModelInfoProvider ë“±
```

### í•µì‹¬ íŒŒì¼
- **CMake**: `app/src/main/cpp/CMakeLists.txt` â€” `add_subdirectory(llama.cpp)`ë¡œ ì†ŒìŠ¤ í¬í•¨
- **JNI ë¸Œë¦¬ì§€**: `app/src/main/cpp/native-lib.cpp`
- **llama.cpp ì—°ê³„ í´ë˜ìŠ¤(C++)**: `LlamaModel.cpp`, `LlamaGenerationSession.cpp`, `LlamaCpp.h`
- **Kotlin JNI ë˜í¼**: `com.druk.llamacpp.*`
- **ëª¨ë¸ ì¹´íƒˆë¡œê·¸/í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿**: `app/src/main/java/com/druk/lmplayground/models/ModelInfoProvider.kt`

---

## âš™ï¸ 2. ë¹Œë“œ êµ¬ì„±

### Gradle (ì•± ëª¨ë“ˆ) â€” `app/build.gradle.kts`
- **externalNativeBuild**ì—ì„œ CMake ì‚¬ìš© ë° `CMakeLists.txt` ì§€ì •
- **ABI íƒ€ê²Ÿ**: `abiFilters += setOf("arm64-v8a", "x86_64")`
- (ì‹¤ê¸°ê¸° ë°°í¬ëŠ” **arm64-v8aë§Œ** ë‚¨ê¸°ëŠ” ê²ƒì„ ê¶Œì¥)

### Gradle / NDK / CMake ë²„ì „
- **NDK**: `ndkVersion = "27.2.12479018"`
- **CMake**: `version = "3.22.1"`
- Gradle ìŠ¤í¬ë¦½íŠ¸ì—ì„œ externalNativeBuild â†’ cmake â†’ `path`, `version` ì§€ì •

### CMake â€” `app/src/main/cpp/CMakeLists.txt`
- ìƒë‹¨ì— `add_subdirectory(llama.cpp)` â€” ë¦¬í¬ì— llama.cpp ì†ŒìŠ¤ê°€ **ì„œë¸Œë””ë ‰í„°ë¦¬**ë¡œ í¬í•¨ë¨
- ì´í›„ íƒ€ê¹ƒ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ include/ë§í‚¹ ê·œì¹™ ì •ì˜

> âš ï¸ íŒ: ì¶”í›„ ì•± í¬ê¸°/ë¹Œë“œ ì‹œê°„ ìµœì í™”ë¥¼ ìœ„í•´ llama.cppë¥¼ **ì„œë¸Œëª¨ë“ˆ(submodule)** ë¡œ ë‘ê³  í•„ìš”í•œ íŒŒì¼ë§Œ í¬í•¨í•˜ê±°ë‚˜, í”„ë¦¬ë¹ŒíŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤.

---

## ğŸ”— 3. JNI í†µí•© ë°©ì‹

### íŒ¨í‚¤ì§€ ë° í´ë˜ìŠ¤
- Kotlin íŒ¨í‚¤ì§€: **`com.druk.llamacpp`**
- ì£¼ìš” ë˜í¼:
  - `LlamaCpp` â€” **ëª¨ë¸ ë¡œë“œ** ë“± ì§„ì…ì  `external fun loadModel(...)`
  - `LlamaModel` â€” ëª¨ë¸ í•¸ë“¤/ì†ì„±/ì„¸ì…˜ ìƒì„±
  - `LlamaGenerationSession` â€” **í…ìŠ¤íŠ¸ ìƒì„±(inference) ì„¸ì…˜**

### ë„¤ì´í‹°ë¸Œ ì‹œê·¸ë‹ˆì²˜(ì˜ˆì‹œ)
JNI í•¨ìˆ˜ë“¤ì€ `native-lib.cpp`ì— êµ¬í˜„ë˜ì–´ ìˆê³ , ì‹œê·¸ë‹ˆì²˜ëŠ” ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.
```
Java_com_druk_llamacpp_LlamaCpp_loadModel(...)
Java_com_druk_llamacpp_LlamaModel_getModelSize(...)
Java_com_druk_llamacpp_LlamaGenerationSession_generate(...)
Java_com_druk_llamacpp_LlamaGenerationSession_destroy(...)
```
- `LlamaGenerationSession_generate`ì—ì„œëŠ” ì½œë°± ê°ì²´ì˜ `newTokens(byte[])` **ë©”ì„œë“œë¥¼ í˜¸ì¶œ**í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° í† í°ì„ ì „ë‹¬í•©ë‹ˆë‹¤.

### í˜¸ì¶œ íë¦„(ìˆœì„œë„)
Kotlin(UI) â†’ `LlamaCpp.loadModel` â†’ JNI â†’ `LlamaModel.cpp`ì—ì„œ `llama_model_load_from_file`  
â†“  
Kotlin(UI) â†’ `LlamaModel.createSession` â†’ JNI â†’ `LlamaGenerationSession.init(model)` â†’ `llama_context_default_params()` ë“± ì´ˆê¸°í™”  
â†“  
Kotlin(UI) â†’ `LlamaGenerationSession.generate(prompt, callback)` â†’ JNI â†’ í† í° ìƒì„± ë£¨í”„ì—ì„œ `callback.newTokens(bytes)`ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì „ë‹¬

---

## ğŸ§  4. ëª¨ë¸ ë¡œë”© ê³¼ì •

### íŒŒì¼/ê²½ë¡œ
- ëª¨ë¸ íŒŒì¼ì€ **`.gguf`** í˜•ì‹ ì‚¬ìš©
- JNI: `LlamaModel.cpp` ë‚´ë¶€ì—ì„œ
  - `llama_model_params model_params = llama_model_default_params();`
  - `llama_model_load_from_file(modelPath.c_str(), model_params);`

### ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
- `LlamaGenerationSession.cpp`ì—ì„œ
  - `llama_context_params ctx_params = llama_context_default_params();`
  - `llama_model_get_vocab(model)` ë“±ìœ¼ë¡œ vocab íšë“
  - ìƒ˜í”ŒëŸ¬/ë°°ì¹˜/ë©”ì‹œì§€ ë²„í¼ ì¤€ë¹„

### ìƒì„±(inference)
- ì„¸ì…˜ì˜ `generate()`ì—ì„œ í”„ë¡¬í”„íŠ¸ í† í°í™” â†’ ë””ì½”ë”© ë£¨í”„
- JNI ì¸¡ì—ì„œ **ìŠ¤íŠ¸ë¦¬ë° ì½œë°±**(`newTokens(byte[])`)ìœ¼ë¡œ UIì— í† í°ì„ ì „ë‹¬

### ì–¸ë¡œë“œ/ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- `LlamaGenerationSession_destroy(...)`ì—ì„œ ë„¤ì´í‹°ë¸Œ ê°ì²´ ì‚­ì œ ë° í•¸ë“¤ null ì²˜ë¦¬
- ëª¨ë¸ í•´ì œëŠ” `llama_model_free(model)` ê²½ë¡œ ì‚¬ìš©

---

## ğŸ’¬ 5. ì•± UI â†” Native ìƒí˜¸ì‘ìš©

### Kotlin ì¸í„°í˜ì´ìŠ¤
- **ì½œë°±**: `com.druk.llamacpp.LlamaGenerationCallback`
  ```kotlin
  interface LlamaGenerationCallback {
      fun newTokens(newTokens: ByteArray) // UTF-8 í…ìŠ¤íŠ¸ í† í° ìŠ¤íŠ¸ë¦¼
  }
  ```
- **í”„ë¡œê·¸ë ˆìŠ¤**: `LlamaProgressCallback` â€” ëª¨ë¸ ë¡œë”© ì§„í–‰ë¥ (float 0.0~1.0)

### í˜¸ì¶œ ì˜ˆì‹œ(ê°œë…)
```kotlin
val llama = LlamaCpp()
val model = llama.loadModel(
    path = "/storage/emulated/0/Download/YourModel.gguf",
    inputPrefix = "<|start_header_id|>user<|end_header_id|>\n\n",
    inputSuffix = "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    antiPrompt = arrayOf("<|eot_id|>"),
    progressCallback = object : LlamaProgressCallback {
        override fun onProgress(progress: Float) { /* update UI */ }
    }
)

val session = model.createSession()
session.generate("Hello", object : LlamaGenerationCallback {
    override fun newTokens(newTokens: ByteArray) {
        val text = newTokens.toString(Charsets.UTF_8)
        // append to UI
    }
})
```

- UIëŠ” **ì½”ë£¨í‹´/Dispatchers.Main** ë“±ìœ¼ë¡œ ìŠ¤ë ˆë“œ ì „í™˜í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê°±ì‹ í•˜ì„¸ìš”.

---

## ğŸš€ 6. ë‚´ ì•±ì— 0ë¶€í„° í†µí•©í•˜ê¸° (ì²´í¬ë¦¬ìŠ¤íŠ¸)

1) **llama.cpp ì†ŒìŠ¤ í¬í•¨**
- ë°©ë²• A: í˜„ì¬ ì˜ˆì‹œì²˜ëŸ¼ `app/src/main/cpp/llama.cpp/`ë¡œ í¬í•¨í•˜ê³  `add_subdirectory(llama.cpp)`
- ë°©ë²• B: git submoduleë¡œ ì¶”ê°€ í›„ ë™ì¼ ê²½ë¡œ(or ì›í•˜ëŠ” ê²½ë¡œ)ë¡œ ì—°ê²°

2) **CMake ì„¸íŒ…**
- `CMakeLists.txt`ì— íƒ€ê¹ƒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„± í›„ llama.cpp ì†ŒìŠ¤ì™€ include ê²½ë¡œ ì—°ê²°
- í•„ìš” ì‹œ `LLAMA_*` ì˜µì…˜ë“¤(ì˜ˆ: BLAS/Vulkan) ë¹Œë“œ í”Œë˜ê·¸ ì¡°ì •

3) **Gradle/NDK ì„¸íŒ…**
- `externalNativeBuild.cmake.path = "app/src/main/cpp/CMakeLists.txt"`
- `ndkVersion = "27.2.12479018"` (í”„ë¡œì íŠ¸ ë£¨íŠ¸/ì•±ì— ëª…ì‹œ)
- `abiFilters`ëŠ” ë°°í¬ íƒ€ê²Ÿ ê¸°ì¤€ìœ¼ë¡œ ìµœì†Œí™”(ì˜ˆ: `arm64-v8a`ë§Œ)

4) **JNI ë˜í¼ ì¶”ê°€**
- `com.druk.llamacpp` íŒ¨í‚¤ì§€ êµ¬ì¡°ì™€ ìœ ì‚¬í•˜ê²Œ `external fun` ì„ ì–¸
- C++ ì¸¡ `native-lib.cpp`ì— `Java_com_yourpkg_...` ë„¤ì´ë°ìœ¼ë¡œ êµ¬í˜„

5) **ëª¨ë¸ ë°°ì¹˜**
- ê°œë°œ ë‹¨ê³„: `/sdcard/Download/*.gguf` ê°™ì´ ì ‘ê·¼ ì‰¬ìš´ ìœ„ì¹˜ ì‚¬ìš©
- ìš´ì˜ ë‹¨ê³„: ì²« ì‹¤í–‰ ì‹œ **ë‹¤ìš´ë¡œë“œ â†’ ì•± ì „ìš© ë””ë ‰í„°ë¦¬**ë¡œ ì´ë™/ê²€ì¦ ê¶Œì¥
- ëª¨ë¸ë³„ **inputPrefix/inputSuffix/antiPrompt**ë¥¼ `ModelInfoProvider`ì²˜ëŸ¼ ê´€ë¦¬

6) **inference í…ŒìŠ¤íŠ¸**
- ê°„ë‹¨í•œ UIë¡œ prompt ì…ë ¥ â†’ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í™•ì¸
- ë¡œê·¸ë¡œ `llama_perf_context_print` ê²°ê³¼ë¥¼ í™•ì¸í•˜ì—¬ ì†ë„/í† í° ì²˜ë¦¬ëŸ‰ ì ê²€

---

## âš¡ 7. ì„±ëŠ¥ ë° ìµœì í™” íŒ

- **ì–‘ìí™”(Quantization)**: `.gguf`ì˜ `Q4_K_M`, `Q5`, `Q8` ë“±ìœ¼ë¡œ ë©”ëª¨ë¦¬/ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
- **ABI**: ì‹¤ì œ ê¸°ê¸°ëŠ” `arm64-v8a`ë§Œ ë¹Œë“œí•˜ì—¬ APK í¬ê¸° ì¶•ì†Œ
- **ìŠ¤ë ˆë”©**: `n_threads`(ìŠ¤ë ˆë“œ ìˆ˜) â†’ big.LITTLE êµ¬ì¡°ì—ì„œ ìµœì ê°’ ì‹¤ì¸¡
- **ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©**: ê°™ì€ ì„¸ì…˜ì—ì„œì˜ ë‹¤íšŒ í˜¸ì¶œì€ ìƒì„± ë¹„ìš© ê°ì†Œ
- **ë°±ì—”ë“œ ê°€ì†**: ë¹Œë“œ ì˜µì…˜ìœ¼ë¡œ Vulkan/OpenCL/NEON/Arm Compute Library ê³ ë ¤
- **í† í° ìŠ¤íŠ¸ë¦¬ë°**: UIì—ì„œ ë¶€ë¶„ ë¦¬ë Œë”ë§(append-only)ë¡œ **jank ìµœì†Œí™”**
- **ë©”ëª¨ë¦¬**: ëŒ€ìš©ëŸ‰ ëª¨ë¸ì€ `llama_model_size()` í™•ì¸ í›„ OOM ë°©ì§€ë¥¼ ìœ„í•œ UX ì„¤ê³„

---

## ğŸ§­ 8. í™•ì¥ ì•„ì´ë””ì–´

- **ì˜¨ë””ë°”ì´ìŠ¤ ì±— UI**: ë©€í‹°í„´ ëŒ€í™” ë²„í¼(`messages`)ë¥¼ JNIë¡œ ìœ ì§€í•˜ê³ , ì‹œìŠ¤í…œ/ìœ ì €/ì–´ì‹œìŠ¤í„´íŠ¸ ì—­í•  í† í°ì„ `inputPrefix/suffix`ë¡œ ì—„ë°€íˆ ì²˜ë¦¬
- **Streaming UX**: stop tokens(`antiPrompt`) ê¸°ë°˜ ì¤‘ë‹¨, â€œìƒì„± ì¤‘ì§€â€ ë²„íŠ¼ ì§€ì›
- **í”„ë¡¬í”„íŠ¸ íŒŒì´í”„ë¼ì¸**: í…œí”Œë¦¿ í´ë˜ìŠ¤í™”(ì˜ˆ: Llama3, Qwen, Gemma ì„¸íŠ¸), í† í¬ë‚˜ì´ì € ì˜µì…˜ ë…¸ì¶œ
- **ëª¨ë¸ ë§¤ë‹ˆì €**: ë‹¤ìš´ë¡œë“œ/ê²€ì¦/ì²´í¬ì„¬/ë²„ì „ êµì²´ UI ì œê³µ

---

## ğŸ”š ë¶€ë¡: ì‹¤ì œ íŒŒì¼ ë ˆí¼ëŸ°ìŠ¤
- JNI ë¸Œë¦¬ì§€: `app/src/main/cpp/native-lib.cpp`
- ëª¨ë¸ ë¡œë“œ: `app/src/main/cpp/LlamaModel.cpp` â†’ `llama_model_load_from_file(...)`
- ì»¨í…ìŠ¤íŠ¸: `app/src/main/cpp/LlamaGenerationSession.cpp` â†’ `llama_context_default_params()`
- Kotlin ë˜í¼ íŒ¨í‚¤ì§€: `app/src/main/java/com/druk/llamacpp/`
- Gradle: `app/build.gradle.kts` (ì˜ˆ: `abiFilters += setOf("arm64-v8a", "x86_64")`, CMake `version "3.22.1"`)
- NDK: `ndkVersion = "27.2.12479018"`

---

â€» ë³¸ ê°€ì´ë“œëŠ” ì—…ë¡œë“œëœ LMPlayground í”„ë¡œì íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
